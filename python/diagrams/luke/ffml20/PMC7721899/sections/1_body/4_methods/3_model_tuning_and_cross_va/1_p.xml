<?xml version="1.0" encoding="UTF-8"?>
<p id="Par44">To avoid the possibility of overfitting the data, 20% of the data is set aside from the models for its intended use as a “test set” to assess the performance of the ML algorithms on these unseen data. To this end, a 
 <italic>k</italic>-fold cross-validation (CV) technique is adopted in this study. In the CV technique, the dataset is split into 
 <italic>k</italic> number of smaller sets, where in each fold the model is trained on a fraction of data (train set) and tested on the remaining data. The final value obtained is the average value which is iteratively run on each of the 
 <italic>k</italic>-folds. To this end, this study adopts a nested two-level CV approach as detailed in the article by Cawley and Talbot
 <sup>
  <xref ref-type="bibr" rid="CR56">56</xref>
 </sup>. First, the dataset is split into the training set (which is 80% of the data) and test set (20% of the data). In outer CV the model is run for the number of iterations and the average value of the scores (i.e. 
 <inline-formula id="IEq30">
  <alternatives>
   <tex-math id="M79">\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$R^{2}$$\end{document}</tex-math>
   <math id="M80">
    <msup>
     <mi>R</mi>
     <mn>2</mn>
    </msup>
   </math>
   <inline-graphic xlink:href="41598_2020_78368_Article_IEq30.gif" xmlns:xlink="http://www.w3.org/1999/xlink"/>
  </alternatives>
 </inline-formula> and MSE) obtained from each fold is used to obtain a comparative performance-evaluation of various ML techniques. In order to obtain the appropriate hyperparameters, a fivefold inner CV is implemented for the training dataset. This nested CV technique alleviates some of the issues regarding the limitations of relatively smaller datasets.
</p>
